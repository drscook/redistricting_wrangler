{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drscook/redistricting_wrangler/blob/main/redistricting_2025_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advances in Data Engineering for Redistricting\n",
        "\n",
        "Scott Cook\n",
        "\n",
        "Tarleton State Univ\n",
        "\n",
        "2025-10-13"
      ],
      "metadata": {
        "id": "qxfP1UtutPTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Background"
      ],
      "metadata": {
        "id": "JqfVY27utLcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Geography"
      ],
      "metadata": {
        "id": "ZsrxELritHgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider [Geographic Hierarchy](https://www2.census.gov/geo/pdfs/reference/geodiagram.pdf). We need data from 4 sources that live at different levels:\n",
        "- 2020 Decennial Census PL94-171\n",
        "    - level: block (only dataset at this smallest level)\n",
        "    - https://www.census.gov/programs-surveys/decennial-census/about/rdo/summary-files.html\n",
        "    - For convenience, we're actually pulling from TX Legislative Council: https://data.capitol.texas.gov/dataset/2020-census-geography    \n",
        "- American Community Surveys (ACS)\n",
        "    - level: some block group, some tract\n",
        "    - https://www.census.gov/programs-surveys/acs/data.html\n",
        "    - Like an annual mini-Census\n",
        "    - Use 5 year estimates (more stable)\n",
        "- Citizen Voting Age Population (CVAP) special tabulation\n",
        "    - level: block group\n",
        "    - https://www.census.gov/programs-surveys/decennial-census/about/voting-rights/cvap.html\n",
        "    - Not a part of decennial census or ACS\n",
        "- Elections\n",
        "    - level: [voting tabulation district (VTD)](https://data.capitol.texas.gov/dataset/vtds)\n",
        "    - https://data.capitol.texas.gov/dataset/comprehensive-election-datasets-compressed-format"
      ],
      "metadata": {
        "id": "Mhp5nFdRuaEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem"
      ],
      "metadata": {
        "id": "LfDfHwCMzQ0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Census-controlled levels are nested, VTDs only respect blocks. In other words, a block lies entirely within exactly one VTD. But a block group or tract might be split across mutliple VTDs (and conversely)."
      ],
      "metadata": {
        "id": "pfmAV8vlzURe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Solution"
      ],
      "metadata": {
        "id": "prZcsphHzZuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push all data down to common refinement (pieces that do not cross any boundary) then aggregate back up to desired level\n",
        "1. Fetch raw data from relevant sources\n",
        "1. Geospatially intersect (overlay) block and VTD geometries to create *pieces* (essentially blocks with slight correction for geospatial misalignments)\n",
        "1. Apportion data to pieces proportional to voting age population from 2020 Census (recall 2020 Census is the only data at block level)\n"
      ],
      "metadata": {
        "id": "ra_v1AJ4zbxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Links"
      ],
      "metadata": {
        "id": "6xLAID7Kt4PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Census\n",
        "    - API Key: https://api.census.gov/data/key_signup.html\n",
        "    - Geographic Hierarchy: https://www2.census.gov/geo/pdfs/reference/geodiagram.pdf\n",
        "    - Geoids: https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html\n",
        "    - American Community Survey (ACS): https://www.census.gov/programs-surveys/acs/data.html\n",
        "    - ACS variables: https://api.census.gov/data/2023/acs/acs5/variables.html\n",
        "    - Citizen Voting Age Population (CVAP): https://www.census.gov/programs-surveys/decennial-census/about/voting-rights/cvap.html\n",
        "    - Block shapefiles: https://www2.census.gov/geo/tiger/TIGER2020/TABBLOCK20/\n",
        "    - Python: https://pypi.org/project/census/\n",
        "\n",
        "- Texas Legislative Council\n",
        "    - Voting Tabulation Districts (VTD): https://data.capitol.texas.gov/dataset/vtds\n",
        "    - Elections: https://data.capitol.texas.gov/dataset/comprehensive-election-datasets-compressed-format\n",
        "    - PL94-171: https://data.capitol.texas.gov/dataset/2020-census-geography\n",
        "    - School Districts (not used here): https://data.capitol.texas.gov/dataset/school-districts\n",
        "\n",
        "- Other\n",
        "    - CRS:\n",
        "        - https://docs.qgis.org/3.40/en/docs/gentle_gis_introduction/coordinate_reference_systems.html\n",
        "        - https://epsg.io/3085\n",
        "        - https://epsg.io/4269\n",
        "    - Tarrant precincts: https://www.tarrantcountytx.gov/en/elections/interactive-maps/commissioner-precinct-maps.html\n",
        "\n",
        "- Quesion: Did Andrea find block assignment files for on TX Lege data portal? If so, where? Census has BAF for congressional, senate, and house here, but I think she has a source with more (school districts, judicial, etc).\n",
        "\n"
      ],
      "metadata": {
        "id": "H82q4fLVa4zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "8C2XzSfyt94O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Request Census API key\n",
        "    - https://api.census.gov/data/key_signup.html\n",
        "    - path into api_key = '___' below\n",
        "- Run cells below once at the start of session\n",
        "    - installs packages\n",
        "    - ignore pop-up about \"session crash for unknown reason\"\n",
        "    - mounts your google drive to save data\n",
        "        - need to confirm through several pop-ups    \n",
        "        - click folder icon on left (under key icon)\n",
        "        - navigate drive > MyDrive to where you want to save data\n",
        "        - hover over it, click 3 dots on right, click \"copy path\"\n",
        "        - paste into root = pathlib.Path('___') below"
      ],
      "metadata": {
        "id": "N4CKZrexa_y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run once to begin each session\n",
        "%pip install -U ipython-autotime Census us\n",
        "from IPython import get_ipython\n",
        "get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "id": "BHjKl75-bwrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DaUEorlibakv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "-Hdof_yMuBNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#districts - done\n",
        "#adjacency matrix - done\n",
        "#link to ACS variable list - done (in Links section)\n",
        "#parquet to csv function - done\n",
        "#universe variable - done\n",
        "#list of offices - done (optional argument to get_elections)\n",
        "\n",
        "%reload_ext autotime\n",
        "import pathlib, requests, zipfile, pickle, warnings, census, us, pyarrow.parquet as pq\n",
        "import numpy as np, pandas as pd, scipy as sp, geopandas as gpd, networkx as nx\n",
        "from shapely.strtree import STRtree\n",
        "root = pathlib.Path('/content/drive/MyDrive/redistricting_2025_10')/'data'\n",
        "api_key = '5640e76608e24d8d6cc35b96ce35028445957cb5'\n",
        "session = census.Census(api_key)\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings(\"ignore\", message=\"Allowing arbitrary scalar fill_value in SparseDtype is deprecated\", category=FutureWarning)\n",
        "\n",
        "#################### helpers ####################\n",
        "def prep(df):\n",
        "    return df.convert_dtypes().rename(columns=lambda x: x.strip().lower().replace(' ','_').replace('-','_'))\n",
        "\n",
        "\n",
        "def dump(file, obj, **kwargs):\n",
        "    \"\"\"write obj to file\"\"\"\n",
        "    file.parent.mkdir(parents=True, exist_ok=True)  #make parent directory\n",
        "    #write using method associated to file.suffix\n",
        "    if file.suffix == '.parquet':\n",
        "        prep(obj).to_parquet(file, **kwargs)\n",
        "    elif file.suffix in ['.csv', '.txt']:\n",
        "        prep(obj).to_csv(file, **kwargs)\n",
        "    elif file.suffix == '.npz':\n",
        "        numpy.savez_compressed(file, obj, **kwargs)\n",
        "    else:\n",
        "        with open(file, 'wb') as f:\n",
        "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL, **kwargs)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def load(file, **kwargs):\n",
        "    \"\"\"read obj from file\"\"\"\n",
        "    if file.suffix == '.parquet':\n",
        "        if any(col.type=='binary' for col in pq.ParquetFile(file).schema_arrow):  #if parquet contains geometry columns, use geopandas\n",
        "            return prep(gpd.read_parquet(file, **kwargs))\n",
        "        else:\n",
        "            return prep(pd.read_parquet(file, **kwargs))\n",
        "    elif file.suffix in ['.csv', '.txt']:\n",
        "        return prep(pd.read_csv(file, **kwargs))\n",
        "    elif file.suffix == '.npz':\n",
        "        return np.load(file, allow_pickle=True, **kwargs)\n",
        "    else:\n",
        "        try:\n",
        "            return prep(gpd.read_file(file, **kwargs))  #try geopandas to test is file is geospatial (like shapefile)\n",
        "        except:\n",
        "            with open(file, 'rb') as f:\n",
        "                return pickle.load(f, **kwargs)\n",
        "\n",
        "\n",
        "def parquet_to_csv(src):\n",
        "    assert src.suffix == '.parquet'\n",
        "    dst = src.with_suffix('.csv')\n",
        "    df = load(src)\n",
        "    dump(dst, df)\n",
        "    return df\n",
        "\n",
        "\n",
        "def fetch(file, url, unzip=True):\n",
        "    \"\"\"fetch data from url and write to file\"\"\"\n",
        "    if not file.exists():\n",
        "        print(f'fetching {url} to {file}')\n",
        "        with requests.get(url, stream=True) as response:  #request url:\n",
        "            response.raise_for_status()\n",
        "            file.parent.mkdir(parents=True, exist_ok=True)  #make parent directory\n",
        "            with open(file, 'wb') as f:  #pull data streaming\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "        if unzip and zipfile.is_zipfile(file):  #unzip\n",
        "            with zipfile.ZipFile(file, 'r') as f:\n",
        "                f.extractall(file.parent)\n",
        "    return file\n",
        "\n",
        "\n",
        "def move_col(df, col='geometry', loc=-1):\n",
        "    \"\"\"move col to position loc\"\"\"\n",
        "    df.insert(loc+df.shape[1]*(loc<0), col, df.pop(col))\n",
        "    return df\n",
        "\n",
        "\n",
        "def simplify(df, tolerance=0):\n",
        "    return df.assign(geometry=df.geometry.buffer(0))#.simplify(tolerance, preserve_topology=True))\n",
        "\n",
        "\n",
        "geoid_structure = {'state':2, 'county':3, 'tract':6, 'block_group':1, 'block':4}\n",
        "def make_geoid(df, level):\n",
        "    \"\"\"make geoid from separate columns\"\"\"\n",
        "    df['geoid'] =''\n",
        "    g = lambda col, width: df.pop(col).astype(str).str.rjust(width, '0')  #prepend leading 0's to ensure correct string length\n",
        "    for col, width in geoid_structure.items():\n",
        "        df['geoid'] += g(col, width)  #iteratively append to existing geoid\n",
        "        if col == level:  #stop at level and rename geoid to level\n",
        "            df[level] = df.pop('geoid').astype('Int64')\n",
        "            return df\n",
        "\n",
        "\n",
        "def get_paths(stem):\n",
        "    path = root/stem\n",
        "    dst = path/f'{stem.replace('/', '_')}.parquet'\n",
        "    src = dst.with_suffix('.zip')\n",
        "    return path, dst, src\n",
        "#################### data ####################\n",
        "\n",
        "def get_block():\n",
        "    \"\"\"fetch & process block geometries\"\"\"\n",
        "    path, dst, src = get_paths(f'block/2020/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://www2.census.gov/geo/tiger/TIGER2020/TABBLOCK20/tl_2020_{state.fips}_tabblock20.zip', unzip=False)  #download raw data\n",
        "        print(f'creating {dst}')\n",
        "        gdf = load(src, columns=['GEOID20'])\n",
        "        gdf['block'] = gdf.pop('geoid20').astype('Int64')\n",
        "        dump(dst, gdf)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_vtd():\n",
        "    \"\"\"fetch & process voting tabulation district geometries - only works for TX\"\"\"\n",
        "    path, dst, src = get_paths(f'vtd/{tx_lege_year}/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://data.capitol.texas.gov/dataset/4d8298d0-d176-4c19-b174-42837027b73e/resource/906f47e4-4e39-4156-b1bd-4969be0b2780/download/vtds_pg.zip', unzip=False)  #download raw data\n",
        "        print(f'creating {dst}')\n",
        "        gdf = load(src, columns=['VTDKEY'])\n",
        "        dump(dst, gdf)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_district(district):\n",
        "    \"\"\"fetch & process district block equivalency\"\"\"\n",
        "    path, dst, src = get_paths(f'district/{district}/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        url = 'https://data.capitol.texas.gov/dataset/b806b39a-4bab-4103-a66a-9c99bcaba490/resource/c3f03464-e320-4d7f-b528-1883bd82cfb2/download/planc2193_blk.zip' if district == 'congress' else \\\n",
        "              'https://data.capitol.texas.gov/dataset/70836384-f10c-423d-a36e-748d7e000872/resource/0af4cde1-f651-4e9f-aded-a47a88cb5548/download/plans2168_blk.zip' if district == 'senate' else \\\n",
        "              'https://data.capitol.texas.gov/dataset/71af633c-21bf-42cf-ad48-4fe95593a897/resource/fbba5db7-0b1a-4eee-8d65-375c4d092b62/download/planh2316_blk.zip' if district == 'house' else None\n",
        "        fetch(src, url)\n",
        "        print(f'creating {dst}')\n",
        "        df = load(src.parent/f'{url.split(\"/\")[-1].split(\"_\")[0].upper()}.csv')\n",
        "        df.columns = ['block', district]\n",
        "        dump(dst, df)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_pl94():\n",
        "    \"\"\"fetch & process PL94-171 - only works for TX (currently)\"\"\"\n",
        "    path, dst, src = get_paths(f'pl94/2020/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://data.capitol.texas.gov/dataset/2b59f5ce-5fa4-4040-a550-caffbe8986c4/resource/33ed77c5-951b-4a88-9de7-9c90c4ba50db/download/blocks_pop.zip')  #download raw data\n",
        "        print(f'creating {dst}')\n",
        "        repl = {'sctbkey':'block', 'fename':'county'} | {k:f'{k}_2020' for k in ['anglo','asian','hisp','total','vap','black','bh','nanglo','anglovap','hispvap','bhvap','blackvap','asianvap','nanglovap']}\n",
        "        df = load(src.parent/'Blocks_Pop.txt')[repl.keys()].rename(columns=repl)\n",
        "        dump(dst, df)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_piece():\n",
        "    \"\"\"intersect geometries into pieces that do not cross any boundary\"\"\"\n",
        "    path, dst, src = get_paths(f'piece/2024/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        P = get_pl94()\n",
        "        V = simplify(get_vtd())\n",
        "        B = simplify(get_block().to_crs(V.crs))\n",
        "        print(f'creating {dst}')\n",
        "        #according to Texas Legislative Council, vtds are specifically created so each block is wholly contained in exactly 1 vtd\n",
        "        #however, there can be tiny precision errors, so we assign each block to the vtd with largest intersection by area\n",
        "        I = gpd.overlay(V, B, keep_geom_type=True)  #intersect\n",
        "        I = I.assign(area=I.area).sort_values('area').groupby('block')['vtdkey'].last().reset_index()  #for each block, find vtd with largest intersection by area\n",
        "        gdf = B.merge(I)\n",
        "        gdf['block_group'] = gdf['block']//1000\n",
        "        gdf['tract'] = gdf['block']//10000\n",
        "        for district in ['congress','senate','house']:\n",
        "            gdf = gdf.merge(get_district(district))  #merge districts\n",
        "        gdf = move_col(gdf.merge(P))  #merge pl94-171 data\n",
        "        dump(dst, gdf)\n",
        "    return load(dst).set_index(['block','block_group','tract','vtdkey',*districts])  #include additional districts here too\n",
        "\n",
        "\n",
        "def get_multiplier(level):\n",
        "    \"\"\"compute multipliers to apportion from level to blocks via universe\"\"\"\n",
        "    path, dst, src = get_paths(f'multiplier/2020/{state.abbr}/{level}/{universe}')\n",
        "    if not dst.exists():\n",
        "        P = get_piece()\n",
        "        print(f'creating {dst}')\n",
        "        #for each piece, compute its universe / total universe in the unit that contains it\n",
        "        T = P.groupby(level)[universe].transform('sum').clip(1)  #we will get 0/0 errors below iff ALL pieces in a given unit have universe = 0; clip(1) applies lower bound = 1 to the unit's value to avoid\n",
        "        df = (P[universe]/T).rename('multiplier').to_frame()\n",
        "        dump(dst, df)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def apportion(data, level):\n",
        "    \"\"\"apportion data from level to blocks\"\"\"\n",
        "    data = data.set_index(level)\n",
        "    df = get_multiplier(level).join(data)  #join multiplier to data\n",
        "    df *= df.pop('multiplier').to_frame().values  #pop multiplier and use it to multiply all remaining columns\n",
        "    # check that we recover the original values by aggregating back to original level\n",
        "    chk = data - df.groupby(level).sum()\n",
        "    assert np.max(np.abs(chk) < 0.001)\n",
        "    return df.squeeze()\n",
        "\n",
        "\n",
        "def get_acs_yr_code(yr, code):\n",
        "    \"\"\"fetch & process 1 acs variable for 1 year\"\"\"\n",
        "    path, dst, src = get_paths(f'acs/{yr}/{state.abbr}/{code}')\n",
        "    if not dst.exists():\n",
        "        print(f'creating {dst}')\n",
        "        #detect whether this acs variable is available at block_group level; if not settle for tracts\n",
        "        test = session.acs5.state_county_blockgroup(code, state.fips, '003', '*', year=yr)  #pull 1 county at block_group level\n",
        "        level = 'block_group' if any(t[code] is not None for t in test) else 'tract'  #any t[code] is not None <=> available for block_group,\n",
        "        #fetch from census api using their python package at the level determined above\n",
        "        if level == 'block_group':\n",
        "            raw = session.acs5.state_county_blockgroup(code, state.fips, '*', '*', year=yr)\n",
        "        else:\n",
        "            raw = session.acs5.state_county_tract(code, state.fips, '*', '*', year=yr)\n",
        "        df = make_geoid(prep(pd.DataFrame(raw)), level)\n",
        "        dump(dst, df)\n",
        "    df = load(dst)\n",
        "    level = df.columns[-1]\n",
        "    return apportion(df, level)\n",
        "\n",
        "\n",
        "def get_acs_yr(yr, acs_variables):\n",
        "    \"\"\"fetch & process all acs variables for 1 year\"\"\"\n",
        "    L = [get_acs_yr_code(yr, code).rename(f'{alias}_{yr}') for code, alias in acs_variables]\n",
        "    return pd.concat(L, axis=1)\n",
        "\n",
        "\n",
        "def get_cvap_yr(yr):\n",
        "    \"\"\"fetch & process cvap for 1 year\"\"\"\n",
        "    path, dst, src = get_paths(f'cvap/{yr}')\n",
        "    level = 'block_group'\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://www2.census.gov/programs-surveys/decennial/rdo/datasets/{yr}/{yr}-cvap/CVAP_{yr-4}-{yr}_ACS_csv_files.zip')  #download raw data\n",
        "        print(f'creating {dst}')\n",
        "        df = load(src.parent/'BlockGr.csv', encoding='latin1')  #load relevant file\n",
        "        df[level] = df['geoid'].str[-12:].astype('Int64')  #create block_group identifier columns\n",
        "        df = df.pivot_table(index=level, columns='lntitle', values=['cit_est','cvap_est'], fill_value=0)  #pivot long to wide so each row is a blkgrp with colunms for each citizen & cvap variable\n",
        "        df.columns = [f'{k[:-4]}_{v}_{yr}' for k,v in df.columns]  #clean up after pivot\n",
        "        dump(dst, df.reset_index())\n",
        "    return apportion(load(dst), level)\n",
        "\n",
        "\n",
        "def get_election(print_offices=0):\n",
        "    \"\"\"get election results - only works for TX\"\"\"\n",
        "    path, dst, src = get_paths(f'election/{tx_lege_year}/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://data.capitol.texas.gov/dataset/35b16aee-0bb0-4866-b1ec-859f1f044241/resource/e1cd6332-6a7a-4c78-ad2a-852268f6c7a2/download/general-vtds-election-data.zip')  #download raw data\n",
        "        print(f'creating {dst}')\n",
        "        L = [load(file, usecols=['vtdkeyvalue','Office','Name','Party','Votes']).assign(year=int(file.stem[:4])) for file in src.parent.iterdir() if 'General_Election_Returns' in file.stem and 'City' not in file.stem]\n",
        "        df = pd.concat(L).rename(columns={'vtdkeyvalue':level})\n",
        "        for k in ['office','name']:\n",
        "            df[k] = df[k].str.replace('_',' ').str.replace('.', '')  #process strings\n",
        "        dump(dst, df)\n",
        "    df = load(dst)\n",
        "    if print_offices:\n",
        "        display(df['office'].value_counts().head(print_offices))\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_vote(years, offices):\n",
        "    \"\"\"process and filter election results - only works for TX\"\"\"\n",
        "    level = 'vtdkey'\n",
        "    df = (\n",
        "        get_election()\n",
        "        .query(f'year in @years and office in @offices and party in [\"D\",\"R\"]', engine='python')  # keep elections for specified offices in specified years for democrats and republicans\n",
        "        .assign(candidate=lambda X: X['office']+'_'+X['name']+'_'+X['party']+'_'+X['year'].astype('string'))  # concat information as column names for pivot\n",
        "        .pivot_table(index=level, columns='candidate', values='votes', fill_value=0)  # pivot long to wide so each row is a vtd with colunms for each election\n",
        "        .rename_axis(columns=None).reset_index()  # clean up after pivot\n",
        "        )\n",
        "    return apportion(df, level)\n",
        "\n",
        "\n",
        "def get_merged(years, offices, acs_variables):\n",
        "    \"\"\"merge all data into pieces\"\"\"\n",
        "    path, dst, src = get_paths(f'merged/{state.abbr}/piece')\n",
        "    if not dst.exists():\n",
        "        L = [\n",
        "            *[get_acs_yr(yr, acs_variables) for yr in years],\n",
        "            *[get_cvap_yr(yr).filter(like='cvap') for yr in years],\n",
        "            get_vote(years, offices),\n",
        "            get_piece(),\n",
        "        ]\n",
        "        print(f'creating {dst}')\n",
        "        gdf = gpd.GeoDataFrame(pd.concat(L, axis=1), geometry='geometry')\n",
        "        dump(dst, gdf)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_aggegrated(level):\n",
        "    \"\"\"aggregate pieces to level\"\"\"\n",
        "    path, dst, src = get_paths(f'merged/{state.abbr}/{level}')\n",
        "    if not dst.exists():\n",
        "        global pieces, years, offices, acs_variables\n",
        "        try:\n",
        "            pieces\n",
        "        except:  #create pieces in globals if it does not already exist\n",
        "            pieces = get_merged(years, offices, acs_variables)\n",
        "        print(f'creating {dst}')\n",
        "        if level == 'block':\n",
        "            gdf = pieces\n",
        "        else:\n",
        "            aggfunc = {k:'last' for k in districts} | {k:'sum' for k in pieces.columns.drop('geometry')}\n",
        "            gdf = move_col(pieces.reset_index(districts).sort_values(universe).dissolve(level, aggfunc, method='coverage'))\n",
        "        dump(dst, gdf)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_graph(gdf, rook=False):\n",
        "    \"\"\"compute adjacency matrix and dual graph\"\"\"\n",
        "    level = gdf.index.names[0]\n",
        "    path, dst, src = get_paths(f'merged/{state.abbr}/{level}')\n",
        "    dst = dst.with_name(f'{dst.stem}_adjacency_{\"rook\" if rook else \"queen\"}.npz')\n",
        "    if not dst.exists():\n",
        "        print(f'creating {dst}')\n",
        "        def check_corner(x, y):\n",
        "            \"\"\"return True if intersection contain a line segment (not a corner)\"\"\"\n",
        "            if rook:  # using rook adjacency so corners are not okay\n",
        "                g = x.intersection(y)\n",
        "                return g.geom_type in ('LineString', 'MultiLineString') or (g.geom_type == 'GeometryCollection' and any(h.geom_type in ('LineString','MultiLineString') for h in g.geoms))\n",
        "            else:  # using queen adjacency so corners are okay\n",
        "                return True\n",
        "        tree = STRtree(gdf.geometry)\n",
        "        #list of geoms that touch\n",
        "        adj = [[i,j] for i, x in enumerate(gdf.geometry) for j in tree.query(x, predicate='touches') if i < j and check_corner(x, gdf.geometry.iloc[j])]\n",
        "        #build adjacency as scipy sparse matrix - standard dense matrix exceeds memory at block level\n",
        "        rows, cols = np.array(adj).T.tolist()\n",
        "        adj = sp.sparse.csr_matrix((np.ones(len(rows)*2, dtype=bool), [rows+cols, cols+rows]), shape=(gdf.shape[0], gdf.shape[0]))\n",
        "        sp.sparse.save_npz(dst, adj)\n",
        "    adj = sp.sparse.load_npz(dst)\n",
        "    #create networkx graph\n",
        "    try:\n",
        "        G = nx.from_scipy_sparse_array(adj, nodelist=gdf.index)  #should replace 2 lines below for networkx >= 3.6 (future release)\n",
        "    except:\n",
        "        G = nx.from_scipy_sparse_array(adj)\n",
        "        G = nx.relabel_nodes(G, dict(enumerate(gdf.index)))\n",
        "    nx.set_node_attributes(G, gdf.drop(columns=\"geometry\").to_dict(\"index\"))\n",
        "    return G, pd.DataFrame.sparse.from_spmatrix(adj, index=gdf.index, columns=gdf.index)\n",
        "\n",
        "\n",
        "state = us.states.TX\n",
        "tx_lege_year = 2024\n",
        "districts = ['county', 'congress', 'senate', 'house']\n",
        "universe = 'vap_2020'  #variable used to apportion data onto pieces\n",
        "\n",
        "years = [2020, 2021, 2022, 2023]\n",
        "acs_variables = [\n",
        "    ['B01001_001E' , 'pop_total'],\n",
        "    ['B01001I_001E', 'hisp_total'],\n",
        "]\n",
        "offices = [\n",
        "    'President',\n",
        "    'US Sen',\n",
        "    'Governor',\n",
        "    'Lt Governor',\n",
        "    'Attorney Gen',\n",
        "]\n",
        "# get_election(print_offices=30)  #print_offices > 0 displays names of offices"
      ],
      "metadata": {
        "id": "3X9yOfIvvTkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "level = 'tract'\n",
        "gdf = get_aggegrated(level)\n",
        "G, adj = get_graph(gdf)#, rook=True)"
      ],
      "metadata": {
        "id": "UXa_1nzZiq2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for level in [\n",
        "    # 'congress',\n",
        "    # 'senate',\n",
        "    # 'house',\n",
        "    # 'county',\n",
        "    # 'tract',\n",
        "    # 'block_group',\n",
        "    'block',\n",
        "    ]:\n",
        "    print(level)\n",
        "    gdf = get_aggegrated(level)\n",
        "    for rook in [True, False]:\n",
        "        G, adj = get_adjacency(gdf, rook)\n"
      ],
      "metadata": {
        "id": "2-wU-lIZVUon"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}