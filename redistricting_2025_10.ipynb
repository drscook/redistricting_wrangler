{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drscook/redistricting_wrangler/blob/main/redistricting_2025_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advances in Data Engineering for Redistricting\n",
        "\n",
        "Scott Cook\n",
        "\n",
        "Tarleton State Univ\n",
        "\n",
        "2025-10-13"
      ],
      "metadata": {
        "id": "qxfP1UtutPTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Background"
      ],
      "metadata": {
        "id": "JqfVY27utLcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Geography"
      ],
      "metadata": {
        "id": "ZsrxELritHgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider [Geographic Hierarchy](https://www2.census.gov/geo/pdfs/reference/geodiagram.pdf). We need data from 4 sources that live at different levels:\n",
        "- 2020 Decennial Census PL94-171\n",
        "    - level: block (only dataset at this smallest level)\n",
        "    - https://www.census.gov/programs-surveys/decennial-census/about/rdo/summary-files.html\n",
        "    - For convenience, we're actually pulling from TX Legislative Council: https://data.capitol.texas.gov/dataset/2020-census-geography    \n",
        "- American Community Surveys (ACS)\n",
        "    - level: some block group, some tract\n",
        "    - https://www.census.gov/programs-surveys/acs/data.html\n",
        "    - Like an annual mini-Census\n",
        "    - Use 5 year estimates (more stable)\n",
        "    \n",
        "- Citizen Voting Age Population (CVAP) special tabulation\n",
        "    - level: block group\n",
        "    - https://www.census.gov/programs-surveys/acs/data.html\n",
        "    - Not a part of decennial census or ACS\n",
        "- Elections\n",
        "    - level: [voting tabulation district (VTD)](https://data.capitol.texas.gov/dataset/vtds)\n",
        "    - https://data.capitol.texas.gov/dataset/comprehensive-election-datasets-compressed-format"
      ],
      "metadata": {
        "id": "Mhp5nFdRuaEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem"
      ],
      "metadata": {
        "id": "LfDfHwCMzQ0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Census-controlled levels are nested, VTDs only respect blocks. In other words, a block lies entirely within exactly one VTD. But a block group or tract might be split across mutliple VTDs (and conversely)."
      ],
      "metadata": {
        "id": "pfmAV8vlzURe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Solution"
      ],
      "metadata": {
        "id": "prZcsphHzZuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push all data down to common refinement (pieces that do not cross any boundary) then aggregate back up to desired level\n",
        "1. Fetch raw data from relevant sources\n",
        "1. Geospatially intersect (overlay) block and VTD geometries to create *pieces* (essentially blocks with slight correction for geospatial misalignments)\n",
        "1. Apportion data to pieces proportional to voting age population from 2020 Census (recall 2020 Census is the only data at block level)\n"
      ],
      "metadata": {
        "id": "ra_v1AJ4zbxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Links"
      ],
      "metadata": {
        "id": "6xLAID7Kt4PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Census\n",
        "    - API Key: https://api.census.gov/data/key_signup.html\n",
        "    - Geographic Hierarchy: https://www2.census.gov/geo/pdfs/reference/geodiagram.pdf\n",
        "    - Geoids: https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html\n",
        "    - American Community Survey (ACS): https://www.census.gov/programs-surveys/acs/data.html\n",
        "    - Citizen Voting Age Population (CVAP): https://www.census.gov/programs-surveys/decennial-census/about/voting-rights/cvap.html\n",
        "    - Block shapefiles: https://www2.census.gov/geo/tiger/TIGER2020/TABBLOCK20/\n",
        "    - Python: https://pypi.org/project/census/\n",
        "\n",
        "- Texas Legislative Council\n",
        "    - Voting Tabulation Districts (VTD): https://data.capitol.texas.gov/dataset/vtds\n",
        "    - Elections: https://data.capitol.texas.gov/dataset/comprehensive-election-datasets-compressed-format\n",
        "    - PL94-171: https://data.capitol.texas.gov/dataset/2020-census-geography\n",
        "    - School Districts (not used here): https://data.capitol.texas.gov/dataset/school-districts\n",
        "\n",
        "- Other\n",
        "    - CRS:\n",
        "        - https://docs.qgis.org/3.40/en/docs/gentle_gis_introduction/coordinate_reference_systems.html\n",
        "        - https://epsg.io/3085\n",
        "        - https://epsg.io/4269\n",
        "    - Tarrant precincts: https://www.tarrantcountytx.gov/en/elections/interactive-maps/commissioner-precinct-maps.html\n",
        "\n",
        "- Quesion: Did Andrea find block assignment files for on TX Lege data portal? If so, where? Census has BAF for congressional, senate, and house here, but I think she has a source with more (school districts, judicial, etc).\n",
        "\n"
      ],
      "metadata": {
        "id": "H82q4fLVa4zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "8C2XzSfyt94O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Request Census API key\n",
        "    1. https://api.census.gov/data/key_signup.html\n",
        "    1. path into api_key = '___' below\n",
        "1. Mount google drive\n",
        "    1. click folder icon on left (under key icon)\n",
        "    1. click folder iocn with inset google drive triangle\n",
        "    1. allow\n",
        "    1. new folder \"drive\" appears in file browser (refresh if not)\n",
        "    1. open it and navigate to where you want to save\n",
        "    1. hover over it, click 3 dots on right, click \"copy path\"\n",
        "    1. paste into root = pathlib.Path('___') below\n",
        "1. Run cell below once at the start of session to install packages"
      ],
      "metadata": {
        "id": "N4CKZrexa_y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run once to begin each session\n",
        "%pip install -U ipython-autotime Census us\n",
        "from IPython import get_ipython\n",
        "get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "id": "BHjKl75-bwrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "-Hdof_yMuBNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autotime\n",
        "import pathlib, requests, zipfile, pickle, census, us, pyarrow.parquet as pq\n",
        "import numpy as np, pandas as pd, geopandas as gpd\n",
        "root = pathlib.Path('/content/drive/MyDrive/redistricting_2025_10')/'data'\n",
        "api_key = '5640e76608e24d8d6cc35b96ce35028445957cb5'\n",
        "session = census.Census(api_key)\n",
        "state = us.states.TX\n",
        "tx_lege_year = 2024\n",
        "\n",
        "#################### helpers ####################\n",
        "def prep(df):\n",
        "    return df.convert_dtypes().rename(columns=lambda x: x.strip().lower().replace(' ','_').replace('-','_'))\n",
        "\n",
        "\n",
        "def dump(file, obj, **kwargs):\n",
        "    \"\"\"write obj to file\"\"\"\n",
        "    file.parent.mkdir(parents=True, exist_ok=True)  #make parent directory\n",
        "    #write using method associated to file.suffix\n",
        "    if file.suffix == '.parquet':\n",
        "        prep(obj).to_parquet(file, **kwargs)\n",
        "    elif file.suffix in ['.csv', '.txt']:\n",
        "        prep(obj).to_csv(file, **kwargs)\n",
        "    else:\n",
        "        with open(file, 'wb') as f:\n",
        "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL, **kwargs)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def load(file, **kwargs):\n",
        "    \"\"\"read obj from file\"\"\"\n",
        "    if file.suffix == '.parquet':\n",
        "        if any(col.type=='binary' for col in pq.ParquetFile(file).schema_arrow):  #if parquet contains geometry columns, use geopandas\n",
        "            return prep(gpd.read_parquet(file, **kwargs))\n",
        "        else:\n",
        "            return prep(pd.read_parquet(file, **kwargs))\n",
        "    elif file.suffix in ['.csv', '.txt']:\n",
        "        return prep(pd.read_csv(file, **kwargs))\n",
        "    else:\n",
        "        try:\n",
        "            return prep(gpd.read_file(file, **kwargs))  #try geopandas to test is file is geospatial (like shapefile)\n",
        "        except:\n",
        "            with open(file, 'rb') as f:\n",
        "                return pickle.load(f, **kwargs)\n",
        "\n",
        "\n",
        "def fetch(file, url, unzip=True):\n",
        "    \"\"\"fetch data from url and write to file\"\"\"\n",
        "    if not file.exists():\n",
        "        print(f'fetching {url} to {file}')\n",
        "        response = requests.get(url)  #request url\n",
        "        assert response.ok\n",
        "        file.parent.mkdir(parents=True, exist_ok=True)  #make parent directory\n",
        "        with open(file, 'wb') as f:\n",
        "            f.write(response.content)  #pull data\n",
        "        if unzip and zipfile.is_zipfile(file):  #unzip\n",
        "            with zipfile.ZipFile(file, 'r') as f:\n",
        "                f.extractall(file.parent)\n",
        "    return file\n",
        "\n",
        "\n",
        "geoid_structure = {'state':2, 'county':3, 'tract':6, 'block_group':1, 'block':4}\n",
        "def make_geoid(df, level):\n",
        "    \"\"\"make geoid from separate columns\"\"\"\n",
        "    df['geoid'] =''\n",
        "    g = lambda col, width: df.pop(col).astype(str).str.rjust(width, '0')  #prepend leading 0's to ensure correct string length\n",
        "    for col, width in geoid_structure.items():\n",
        "        df['geoid'] += g(col, width)  #iteratively append to existing geoid\n",
        "        if col == level:  #stop at level and rename geoid to level\n",
        "            df[level] = df.pop('geoid').astype('Int64')\n",
        "            return df\n",
        "\n",
        "\n",
        "def get_paths(stem):\n",
        "    path = root/stem\n",
        "    dst = path/f'{stem.replace('/', '_')}.parquet'\n",
        "    src = dst.with_suffix('.zip')\n",
        "    return path, dst, src\n",
        "\n",
        "#################### data ####################\n",
        "\n",
        "def get_pl94():\n",
        "    \"\"\"fetch & process PL94-171 - only works for TX (currently)\"\"\"\n",
        "    path, dst, src = get_paths(f'pl94/2020/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://data.capitol.texas.gov/dataset/2b59f5ce-5fa4-4040-a550-caffbe8986c4/resource/33ed77c5-951b-4a88-9de7-9c90c4ba50db/download/blocks_pop.zip')  #download raw data\n",
        "        repl = {'sctbkey':'block', 'fename':'county'} | {k:f'{k}_2020' for k in ['anglo','asian','hisp','total','vap','black','bh','nanglo','anglovap','hispvap','bhvap','blackvap','asianvap','nanglovap']}\n",
        "        df = load(src.parent/'Blocks_Pop.txt')[repl.keys()].rename(columns=repl)\n",
        "        dump(dst, df)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_vtds():\n",
        "    \"\"\"fetch & process voting tabulation districts - only works for TX\"\"\"\n",
        "    path, dst, src = get_paths(f'vtds/{tx_lege_year}/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://data.capitol.texas.gov/dataset/4d8298d0-d176-4c19-b174-42837027b73e/resource/906f47e4-4e39-4156-b1bd-4969be0b2780/download/vtds_pg.zip', unzip=False)  #download raw data\n",
        "        df = load(src, columns=['VTDKEY'])\n",
        "        dump(dst, df)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_blocks():\n",
        "    \"\"\"fetch & process block geometries\"\"\"\n",
        "    path, dst, src = get_paths(f'blocks/2020/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://www2.census.gov/geo/tiger/TIGER2020/TABBLOCK20/tl_2020_{state.fips}_tabblock20.zip', unzip=False)  #download raw data\n",
        "        df = load(src, columns=['GEOID20'])\n",
        "        df['block'] = df.pop('geoid20').astype('Int64')\n",
        "        dump(dst, df)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def get_pieces():\n",
        "    \"\"\"intersect geometries into pieces that do not cross any boundary\"\"\"\n",
        "    path, dst, src = get_paths(f'pieces/2024/{state.abbr}')\n",
        "    if not dst.exists():\n",
        "        V = get_vtds()\n",
        "        B = get_blocks().to_crs(V.crs)\n",
        "        df = gpd.overlay(V, B, keep_geom_type=True)  #intersect\n",
        "        df['area'] = df.area  #compute areas\n",
        "        #according to Texas Legislative Council, vtds are specifically created so each block is wholly contained in exactly 1 vtd\n",
        "        #however, there can be tiny errors due to precision issues, so this discards all but the largest piece from any block\n",
        "        #then we merge 2020 Census data from PL94-171\n",
        "        df = df.loc[df['area']==df.groupby('block')['area'].transform('max')]\n",
        "        df['block_group'] = df['block']//1000\n",
        "        df['tract'] = df['block']//10000\n",
        "        #we can add more districts like congressional, senate, house, school district, etc by joining block equivalency files here\n",
        "        df = df.merge(get_pl94())  #merge pl94 data\n",
        "        dump(dst, df[[*df.columns.drop('geometry'), 'geometry']])  #move geometry column to end for convenience\n",
        "    return load(dst).set_index(['block','block_group','tract','vtdkey'])  #include additional districts here too\n",
        "\n",
        "\n",
        "def get_multipliers(level):\n",
        "    \"\"\"compute multipliers to apportion from level to blocks via vap_2020 from PL94-171\"\"\"\n",
        "    path, dst, src = get_paths(f'multipliers/2020/{state.abbr}/{level}')\n",
        "    if not dst.exists():\n",
        "        P = get_pieces()\n",
        "        #for each block, compute its vap_2020 / total vap_2020 in the {level} that contains it\n",
        "        T = P.groupby(level)['vap_2020'].transform('sum').clip(1)  #vap_202 in the {level}; clip(1) replaces 0->1 to avoid 0/0 below\n",
        "        df = (P['vap_2020']/T).rename('multiplier').to_frame()\n",
        "        dump(dst, df)\n",
        "    return load(dst)\n",
        "\n",
        "\n",
        "def apportion(data, level):\n",
        "    \"\"\"apportion data from level to blocks\"\"\"\n",
        "    data = data.set_index(level)\n",
        "    df = get_multipliers(level).join(data)  #join multiplier to data\n",
        "    df *= df.pop('multiplier').to_frame().values  #pop multiplier and use it to multiply all remaining columns\n",
        "    # check that we recover the original values by aggregating back to original level\n",
        "    chk = data - df.groupby(level).sum()\n",
        "    assert np.max(np.abs(chk) < 0.001)\n",
        "    return df.squeeze()\n",
        "\n",
        "\n",
        "def get_acs_yr_code(yr, code):\n",
        "    \"\"\"fetch & process 1 acs variable for 1 year\"\"\"\n",
        "    path, dst, src = get_paths(f'acs/{yr}/{state.abbr}/{code}')\n",
        "    if not dst.exists():\n",
        "        print(f'fetching {dst}')\n",
        "        #detect whether this acs variable is available at block_group level; if not settle for tracts\n",
        "        test = session.acs5.state_county_blockgroup(code, state.fips, '003', '*', year=yr)  #pull 1 county at block_group level\n",
        "        level = 'block_group' if any(t[code] is not None for t in test) else 'tract'  #any t[code] is not None <=> available for block_group,\n",
        "        #fetch from census api using their python package at the level determined above\n",
        "        if level == 'block_group':\n",
        "            raw = session.acs5.state_county_blockgroup(code, state.fips, '*', '*', year=yr)\n",
        "        else:\n",
        "            raw = session.acs5.state_county_tract(code, state.fips, '*', '*', year=yr)\n",
        "        df = make_geoid(prep(pd.DataFrame(raw)), level)\n",
        "        dump(dst, df)\n",
        "    data = load(dst)\n",
        "    level = data.columns[-1]\n",
        "    return apportion(data, level)\n",
        "\n",
        "\n",
        "def get_acs_yr(yr, acs_variables):\n",
        "    \"\"\"fetch & process all acs variables for 1 year\"\"\"\n",
        "    L = [get_acs_yr_code(yr, code).rename(f'{alias}_{yr}') for code, alias in acs_variables]\n",
        "    return pd.concat(L, axis=1)\n",
        "\n",
        "\n",
        "def get_cvap_yr(yr):\n",
        "    \"\"\"fetch & process cvap for 1 year\"\"\"\n",
        "    path, dst, src = get_paths(f'cvap/{yr}')\n",
        "    level = 'block_group'\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://www2.census.gov/programs-surveys/decennial/rdo/datasets/{yr}/{yr}-cvap/CVAP_{yr-4}-{yr}_ACS_csv_files.zip')  #download raw data\n",
        "        df = load(src.parent/'BlockGr.csv', encoding='latin1')  #load relevant file\n",
        "        df[level] = df['geoid'].str[-12:].astype('Int64')  #create block_group identifier columns\n",
        "        df = df.pivot_table(index=level, columns='lntitle', values=['cit_est','cvap_est'], fill_value=0)  #pivot long to wide so each row is a blkgrp with colunms for each citizen & cvap variable\n",
        "        df.columns = [f'{k[:-4]}_{v}_{yr}' for k,v in df.columns]  #clean up after pivot\n",
        "        dump(dst, df.reset_index())\n",
        "    data = load(dst)\n",
        "    return apportion(data, level)\n",
        "\n",
        "\n",
        "def get_elections(offices, start=2010, end=2030):\n",
        "    \"\"\"get election results - only works for TX\"\"\"\n",
        "    path, dst, src = get_paths(f'elections/{tx_lege_year}/{state.abbr}')\n",
        "    level = 'vtdkey'\n",
        "    if not dst.exists():\n",
        "        fetch(src, f'https://data.capitol.texas.gov/dataset/35b16aee-0bb0-4866-b1ec-859f1f044241/resource/e1cd6332-6a7a-4c78-ad2a-852268f6c7a2/download/general-vtds-election-data.zip')  #download raw data\n",
        "        L = [load(file, usecols=['vtdkeyvalue','Office','Name','Party','Votes']).assign(year=int(file.stem[:4])) for file in src.parent.iterdir() if 'General_Election_Returns' in file.stem and 'City' not in file.stem]\n",
        "        df = pd.concat(L).rename(columns={'vtdkeyvalue':level})\n",
        "        for k in ['office','name']:\n",
        "            df[k] = df[k].str.replace('_',' ').str.replace('.', '')  #process strings\n",
        "        dump(dst, df)\n",
        "    data = (\n",
        "        load(dst)\n",
        "        .query(f'office in @offices and party in [\"D\",\"R\"] and year >= {start} and year <= {end}', engine='python')  # keep elections for specified offices in specified years for democrats and republicans\n",
        "        .assign(candidate=lambda X: X['office']+'_'+X['name']+'_'+X['party']+'_'+X['year'].astype('string'))  # concat information as column names for pivot\n",
        "        .pivot_table(index=level, columns='candidate', values='votes', fill_value=0)  # pivot long to wide so each row is a vtd with colunms for each election\n",
        "        .rename_axis(columns=None).reset_index()  # clean up after pivot\n",
        "        )\n",
        "    return apportion(data, level)\n",
        "\n",
        "\n",
        "acs_variables = [\n",
        "    ['B01001_001E' , 'pop_total'],\n",
        "    ['B01001I_001E', 'hisp_total'],\n",
        "]\n",
        "offices = ['President',\n",
        "           'US Sen',\n",
        "           'Governor',\n",
        "           'Lt Governor',\n",
        "           'Attorney Gen',\n",
        "]\n",
        "years = [2020,2021,2022,2023]\n",
        "A = {yr: get_acs_yr(yr, acs_variables) for yr in years}\n",
        "C = {yr: get_cvap_yr(yr) for yr in years}\n",
        "E = get_elections(offices, start=min(years))"
      ],
      "metadata": {
        "id": "-0jQ4zBPNMEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}